{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpOahw6rjM_9",
        "outputId": "b0de1435-883c-481e-dc4c-e7b4b2a11beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(550, 24145)\n",
            "torch.Size([550, 200])\n",
            "MLP with 2 layers and 3 neurons: 0.4909\n",
            "MLP with 2 layers and 4 neurons: 0.4945\n",
            "MLP with 2 layers and 5 neurons: 0.5164\n",
            "MLP with 2 layers and 6 neurons: 0.4982\n",
            "MLP with 2 layers and 7 neurons: 0.4927\n",
            "MLP with 2 layers and 10 neurons: 0.5018\n",
            "MLP with 2 layers and 20 neurons: 0.5036\n",
            "MLP with 2 layers and 30 neurons: 0.4782\n",
            "MLP with 2 layers and 40 neurons: 0.5018\n",
            "MLP with 3 layers and 3 neurons: 0.5036\n",
            "MLP with 3 layers and 4 neurons: 0.5164\n",
            "MLP with 3 layers and 5 neurons: 0.5091\n",
            "MLP with 3 layers and 6 neurons: 0.4727\n",
            "MLP with 3 layers and 7 neurons: 0.4855\n",
            "MLP with 3 layers and 10 neurons: 0.4782\n",
            "MLP with 3 layers and 20 neurons: 0.4836\n",
            "MLP with 3 layers and 30 neurons: 0.4473\n",
            "MLP with 3 layers and 40 neurons: 0.4909\n",
            "MLP with 4 layers and 3 neurons: 0.5236\n",
            "MLP with 4 layers and 4 neurons: 0.5018\n",
            "MLP with 4 layers and 5 neurons: 0.4836\n",
            "MLP with 4 layers and 6 neurons: 0.5018\n",
            "MLP with 4 layers and 7 neurons: 0.5164\n",
            "MLP with 4 layers and 10 neurons: 0.5400\n",
            "MLP with 4 layers and 20 neurons: 0.5109\n",
            "MLP with 4 layers and 30 neurons: 0.5018\n",
            "MLP with 4 layers and 40 neurons: 0.4945\n",
            "MLP with 5 layers and 3 neurons: 0.5127\n",
            "MLP with 5 layers and 4 neurons: 0.5073\n",
            "MLP with 5 layers and 5 neurons: 0.5036\n",
            "MLP with 5 layers and 6 neurons: 0.5000\n",
            "MLP with 5 layers and 7 neurons: 0.5182\n",
            "MLP with 5 layers and 10 neurons: 0.4673\n",
            "MLP with 5 layers and 20 neurons: 0.5073\n",
            "MLP with 5 layers and 30 neurons: 0.5018\n",
            "MLP with 5 layers and 40 neurons: 0.5255\n",
            "Best accuracy: 0.5400\n",
            "Best parameters: ('MLP', 4, 10)\n",
            "Best parameters: None\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import scipy.io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "\n",
        "# Define the multilayer perceptron (MLP) class\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        # Create a list of linear layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        # Add the first layer (from input to first hidden layer)\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        # Add the remaining hidden layers\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "        # Add the output layer\n",
        "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Loop through the hidden layers\n",
        "        for layer in self.layers[:-1]:\n",
        "            # Apply linear transformation and ReLU activation\n",
        "            x = F.sigmoid(layer(x))\n",
        "            x = nn.Dropout(p=0.2)(x)\n",
        "        # Apply the output layer and sigmoid activation\n",
        "        x = torch.squeeze(torch.sigmoid(self.layers[-1](x)))\n",
        "        return x\n",
        "\n",
        "# Define the radial basis function (RBF) class\n",
        "class RBF(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, radius):\n",
        "        super(RBF, self).__init__()\n",
        "        # Initialize the centers and sigmas of the RBFs randomly\n",
        "        self.centers = nn.Parameter(torch.randn(hidden_size, input_size))\n",
        "        self.sigmas = nn.Parameter(torch.randn(hidden_size)*radius)\n",
        "        # Initialize the output layer\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the pairwise distances between x and centers\n",
        "        dists = torch.cdist(x, self.centers)\n",
        "        # Compute the RBF outputs\n",
        "        rbfs = torch.exp(-self.sigmas * dists)\n",
        "        # Apply the output layer and sigmoid activation\n",
        "        x = torch.squeeze(torch.sigmoid(self.linear(rbfs)))\n",
        "        return x\n",
        "\n",
        "# Load the data (you need to provide your own data here)\n",
        "\n",
        "# Convert the data to tensors\n",
        "\n",
        "X = scipy.io.loadmat('X.mat')['X']\n",
        "X = np.squeeze(X);\n",
        "Xtst = scipy.io.loadmat('Xtst.mat')['Xtst']\n",
        "Xtst = np.squeeze(Xtst);\n",
        "y = scipy.io.loadmat('TrainLabels.mat')['TrainLabels'].T\n",
        "y = np.squeeze(y);\n",
        "y[0] = 1\n",
        "y = np.round(y/2+0.5)\n",
        "Indexes = scipy.io.loadmat('Index.mat')['Index'].T\n",
        "Indexes = np.squeeze(Indexes);\n",
        "X = normalize(X)\n",
        "print(X.shape)\n",
        "n_select = 200\n",
        "X = X[:, Indexes[:n_select]]\n",
        "Xtst = Xtst[:, Indexes[:n_select]]\n",
        "\n",
        "X = torch.from_numpy(X).float()\n",
        "Xtst = torch.from_numpy(Xtst).float()\n",
        "y = torch.from_numpy(y).float()\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "# Define the number of folds for cross validation\n",
        "n_folds = 5\n",
        "\n",
        "# Define the range of number of neurons and layers to iterate over\n",
        "n_neurons = [3, 4,5,6,7, 10, 20, 30, 40]\n",
        "radius = [ 0.1, 0.1, 0.1, 0.1]\n",
        "#n_neurons = [5, 10, 20, 30, 40]\n",
        "n_layers = [2,3,4, 5]\n",
        "\n",
        "# Initialize the best accuracy and the best parameters\n",
        "best_acc_mlp = 0\n",
        "best_params_mlp = None\n",
        "best_acc_rbf = 0\n",
        "best_params_rbf = None\n",
        "model_best = None\n",
        "\n",
        "# Loop over the number of neurons\n",
        "ind = -1\n",
        "for l in n_layers:\n",
        "    # Loop over the number of layers\n",
        "    ind += 1\n",
        "    for n in n_neurons:\n",
        "        # Create a list of hidden sizes for the MLP\n",
        "        hidden_sizes = [n] * l\n",
        "        # Initialize the MLP and RBF models\n",
        "\n",
        "        #rbf = RBF(input_size=X.shape[1], hidden_size=n, output_size=1, radius=radius[ind])\n",
        "        # Initialize the cross validation scores\n",
        "        mlp_scores = []\n",
        "        rbf_scores = []\n",
        "        # Create a KFold object\n",
        "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        # Loop over the folds\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            mlp = MLP(input_size=X.shape[1], hidden_sizes=hidden_sizes, output_size=1)\n",
        "            # Split the data into train and test sets\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            # Define the loss function and the optimizer\n",
        "            criterion = nn.BCELoss()\n",
        "            mlp_optimizer = optim.Adam(mlp.parameters())\n",
        "            #rbf_optimizer = optim.Adam(rbf.parameters())\n",
        "            # Train the MLP model\n",
        "            mlp.train()\n",
        "            for epoch in range(100):\n",
        "                # Zero the parameter gradients\n",
        "                mlp_optimizer.zero_grad()\n",
        "                # Forward pass\n",
        "                mlp_output = mlp(X_train)\n",
        "                # Compute the loss\n",
        "                mlp_loss = criterion(mlp_output, y_train)\n",
        "                # Backward pass and optimize\n",
        "                mlp_loss.backward()\n",
        "                mlp_optimizer.step()\n",
        "            # Train the RBF model\n",
        "            #rbf.train()\n",
        "            #for epoch in range(100):\n",
        "            #    # Zero the parameter gradients\n",
        "            #    rbf_optimizer.zero_grad()\n",
        "                # Forward pass\n",
        "            #    rbf_output = rbf(X_train)\n",
        "                # Compute the loss\n",
        "            #    rbf_loss = criterion(rbf_output, y_train)\n",
        "                # Backward pass and optimize\n",
        "            #    rbf_loss.backward()\n",
        "            #    rbf_optimizer.step()\n",
        "            # Evaluate the MLP model\n",
        "            mlp.eval()\n",
        "            with torch.no_grad():\n",
        "                # Predict the test set\n",
        "                mlp_pred = mlp(X_test)\n",
        "                # Convert the predictions to binary labels\n",
        "                mlp_pred = (mlp_pred > 0.5).float()\n",
        "                # Compute the accuracy score\n",
        "                mlp_score = accuracy_score(y_test, mlp_pred)\n",
        "                # Append the score to the list\n",
        "                mlp_scores.append(mlp_score)\n",
        "            # Evaluate the RBF model\n",
        "         #   rbf.eval()\n",
        "         #   with torch.no_grad():\n",
        "                # Predict the test set\n",
        "          #      rbf_pred = rbf(X_test)\n",
        "                # Convert the predictions to binary labels\n",
        "           #     rbf_pred = (rbf_pred > 0.5).float()\n",
        "                # Compute the accuracy score\n",
        "            #    rbf_score = accuracy_score(y_test, rbf_pred)\n",
        "                # Append the score to the list\n",
        "            #    rbf_scores.append(rbf_score)\n",
        "        # Compute the average scores across the folds\n",
        "        mlp_mean = np.mean(mlp_scores)\n",
        "       # rbf_mean = np.mean(rbf_scores)\n",
        "        # Print the scores\n",
        "        print(f\"MLP with {l} layers and {n} neurons: {mlp_mean:.4f}\")\n",
        "       # print(f\"RBF with r={radius[ind]} and {n} neurons: {rbf_mean:.4f}\")\n",
        "        # Check if the MLP score is better than the best accuracy\n",
        "        if mlp_mean > best_acc_mlp:\n",
        "            # Update the best accuracy and the best parameters\n",
        "            best_acc_mlp = mlp_mean\n",
        "            best_params_mlp = (\"MLP\", l, n)\n",
        "            model_best = mlp\n",
        "        # Check if the RBF score is better than the best accuracy\n",
        "       # if rbf_mean > best_acc_rbf:\n",
        "            # Update the best accuracy and the best parameters\n",
        "         #   best_acc_rbf = rbf_mean\n",
        "         #   best_params_rbf = (\"RBF\", n)\n",
        "# Print the best accuracy and the best parameters\n",
        "print(f\"Best accuracy: {best_acc_mlp:.4f}\")\n",
        "print(f\"Best parameters: {best_params_mlp}\")\n",
        "##print(f\"Best accuracy: {best_acc_rbf:.4f}\")\n",
        "print(f\"Best parameters: {best_params_rbf}\")\n",
        "\n",
        "model_best.eval()\n",
        "with torch.no_grad():\n",
        "    # Predict the test set\n",
        "    mlp_pred = model_best(Xtst)\n",
        "    # Convert the predictions to binary labels\n",
        "    mlp_pred = (mlp_pred > 0.5).float()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Save the tensor as a binary file using torch.save\n",
        "torch.save(mlp_pred, \"tensor.pt\")\n",
        "\n",
        "# Load the tensor from the file using torch.load\n",
        "t = torch.load(\"tensor.pt\")\n",
        "\n",
        "# Convert the tensor to a numpy array using .numpy()\n",
        "t_np = t.numpy()\n",
        "\n",
        "# Convert the numpy array to a dataframe using pd.DataFrame()\n",
        "df = pd.DataFrame(t_np)\n",
        "\n",
        "# Save the dataframe as a csv file using .to_csv()\n",
        "df.to_csv(\"TestLabel_mlp.csv\", index=False)\n",
        "\n",
        "# Load the dataframe from the csv file using pd.read_csv()\n",
        "df = pd.read_csv(\"TestLabel_mlp.csv\")"
      ],
      "metadata": {
        "id": "tZrBp6ILv_KC"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "HbtMERV3wACT"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}